<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>DiCaro.Primo_gruppo.hanks API documentation</title>
<meta name="description" content="CONSEGNA:
* Implementare un sistema basato sulla teoria di Hanks per la costruzione del
significato …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>DiCaro.Primo_gruppo.hanks</code></h1>
</header>
<section id="section-intro">
<p>CONSEGNA:
* Implementare un sistema basato sulla teoria di Hanks per la costruzione del
significato.</p>
<ul>
<li>
<p>Scelto un verbo transitivo (quindi valenza &gt;= 2), recuperare da un corpus
delle istanze in cui viene usato.</p>
</li>
<li>
<p>Effettuare il parsing di queste frasi per identificare i supersensi di
WordNet associati agli argomenti del verbo (subject e object).</p>
</li>
<li>
<p>Calcolare le frequenze di questi supersensi per i due ruoli e stampare le
possibili combinazioni.</p>
</li>
</ul>
<p>SVOLGIMENTO:
* Si è scelto il verbo 'to break', in particolare il presente terza persona singolare.</p>
<ul>
<li>
<p>Il corpus utilizzato è Wikipedia, da cui sono state estratte 3000 frasi, usando sketch engine</p>
</li>
<li>
<p>Per il parsing a dipendenze si è usata la libreria spaCy.</p>
</li>
<li>
<p>Sono state scartate quelle frasi in cui il verbo non presenta entrambi i ruoli
richiesti.</p>
</li>
<li>
<p>I termini che svolgono i ruoli vengono lemmatizzati e si va poi a calcolare
il loro synset migliore tramite WSD (algoritmo di Lesk). Nel caso il soggetto
sia 'he', è necessario forzare il suo synset a 'person.n.01' per evitare che
venga erroneamente riconosciuto come 'elio'.</p>
</li>
<li>
<p>Con questi synset si individua il relativo supersenso, andando a calcolare
poi frequenze e combinazioni possibili.</p>
</li>
<li>
<p>Si verifica anche cosa accade raggruppando le combinazioni con ordine inverso,
poiché probabilmente rappresentanti un uso attivo e passivo del verbo.</p>
</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># TLN_dicaro_1.4
&#34;&#34;&#34;
CONSEGNA:
* Implementare un sistema basato sulla teoria di Hanks per la costruzione del
significato.

* Scelto un verbo transitivo (quindi valenza &gt;= 2), recuperare da un corpus
delle istanze in cui viene usato.

* Effettuare il parsing di queste frasi per identificare i supersensi di
WordNet associati agli argomenti del verbo (subject e object).

* Calcolare le frequenze di questi supersensi per i due ruoli e stampare le
possibili combinazioni.

SVOLGIMENTO:
* Si è scelto il verbo &#39;to break&#39;, in particolare il presente terza persona singolare.

* Il corpus utilizzato è Wikipedia, da cui sono state estratte 3000 frasi, usando sketch engine

* Per il parsing a dipendenze si è usata la libreria spaCy.

* Sono state scartate quelle frasi in cui il verbo non presenta entrambi i ruoli
richiesti.

* I termini che svolgono i ruoli vengono lemmatizzati e si va poi a calcolare
il loro synset migliore tramite WSD (algoritmo di Lesk). Nel caso il soggetto
sia &#39;he&#39;, è necessario forzare il suo synset a &#39;person.n.01&#39; per evitare che
venga erroneamente riconosciuto come &#39;elio&#39;.

* Con questi synset si individua il relativo supersenso, andando a calcolare
poi frequenze e combinazioni possibili.

* Si verifica anche cosa accade raggruppando le combinazioni con ordine inverso,
poiché probabilmente rappresentanti un uso attivo e passivo del verbo.
&#34;&#34;&#34;
from pathlib import Path
import nltk
import re
import pandas as pd
from nltk.corpus import wordnet
import matplotlib.pyplot as plt
from collections import Counter
from DiCaro.Primo_gruppo.utils import lesk


def get_wordnet_pos(tag):
    &#34;&#34;&#34;Map POS tag to first character lemmatize() accepts&#34;&#34;&#34;
    tag_dict = {&#34;J&#34;: wordnet.ADJ,
                &#34;N&#34;: wordnet.NOUN,
                &#34;V&#34;: wordnet.VERB,
                &#34;R&#34;: wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)


def skip_synset_search(pos_tag, t_text):
    &#34;&#34;&#34;
    Helper function for forcing certainty synset to some
    :param pos_tag:
    :return: boolean value, Synset (or None)
    &#34;&#34;&#34;
    # pronouns
    if pos_tag == &#34;PRP&#34;:
        if t_text.lower() in [&#34;he&#34;, &#34;she&#34;]:
            return True, wordnet.synset(&#39;person.n.01&#39;)
        elif t_text.lower() == &#34;it&#34;:
            return True, wordnet.synset(&#39;artifact.n.01&#39;)
        else:
            return False, None
    # Which / That
    # elif pos_tag == &#34;WDT&#34; or pos_tag == &#34;IN&#34;:
    #     return True, wordnet.synset(&#39;artifact.n.01&#39;)
    else:
        return False, None


def iequal(a, b):
    &#34;&#34;&#34;
    Helper method for comparing tuples with string element
    :param a: tuples
    :param b: string
    :return: True if string in tuples
    &#34;&#34;&#34;
    for value in a:
        try:
            sub = re.sub(&#34;\&#39;&#34;, &#34;&#34;, str(value))
            if b == sub:
                return True
        except AttributeError:
            return value == b


def plot_hanks(data, max_v, title):
    &#34;&#34;&#34;
    Plot a horizontal bar graph using pd and matplotlib

    :param data: dictionary to bar plot
    :param max_v: maximum value of labels
    :param title: name of the plot to show
    &#34;&#34;&#34;
    plot_df = pd.DataFrame(data, index=[&#39;quantity&#39;])

    # Sort the value
    plot_df = plot_df.sort_values(by=[&#39;quantity&#39;], axis=1, ascending=False)

    # Take top max_v
    top_max = plot_df.iloc[:, :max_v]

    ax = top_max.plot(kind=&#39;barh&#39;, title=f&#34;Hanks cluster with: {title}&#34;, width=8)
    colors, labels = ax.get_legend_handles_labels()

    # envelop in counter
    data = Counter(data)

    # sort the most coomon labels keep only max_v
    best_legend, order = zip(*data.most_common(max_v))
    zipped = zip(colors, labels)
    colors, labels = zip(*[(color, label) for (color, label) in zipped if iequal(best_legend, label)])

    ax.legend(colors, labels, loc=&#39;best&#39;)
    title_f = title.strip().replace(&#34; &#34;, &#34;_&#34;).replace(&#39;\&#34;&#39;, &#34;&#34;)
    plt.savefig(&#39;output/hanks/{}.png&#39;.format(title_f))
    plt.show()
    print(f&#34;\n{title}&#39;s plot saved in output folder.&#34;)


def update_occurrences(ss1, ss2, dictionary):
    &#34;&#34;&#34;
    Update the dictionary {(s1:s2): counter}
    :param ss1: synset1
    :param ss2: synset2
    :param dictionary: counter dictionary
    &#34;&#34;&#34;
    if ss1 is not None and ss2 is not None:
        # Getting supersenses
        t = (ss1.lexname(), ss2.lexname())

        # Getting frequency
        if t in dictionary.keys():
            dictionary[t] = dictionary[t] + 1
        else:
            dictionary[t] = 1


def fancy_print(title, dictionary):
    &#34;&#34;&#34;
    Helper function for evaluating performance
    :param title: string regarding the strategy
    :param dictionary: the semantic strategy
    &#34;&#34;&#34;
    print(&#39;\n%s:\n\tFinding Semantic Clusters (percentage, count of instances, semantic cluster):&#39; % title)
    for key, value in sorted(dictionary.items(), key=lambda x: x[1]):
        to_print = str(round((value / f_length) * 100, 2))
        print(&#34;\t[{}%] - {} - {}&#34;.format(to_print, value, key))


if __name__ == &#39;__main__&#39;:
    # 1 I choose said, that have a good context in brown
    verb_of_interest = [&#34;breaks&#34;]

    # 2. let&#39;s get a list of sentences with our verb_of_interest in from https://app.sketchengine.eu/
    # already pre-processed
    # path = Path(&#39;.&#39;) / &#39;input&#39; / &#39;concordance_brown_family.csv&#39;
    path = Path(&#39;.&#39;) / &#39;input&#39; / &#39;concordance_enwiki.csv&#39;

    brown_says = pd.read_csv(path)

    print(&#39;[1] - Extracting sentences...&#39;)
    sentences = brown_says[&#34;Sentence&#34;]
    head_corpus = sentences.head(1000)
    head_corpus = head_corpus.apply(lambda x: x.strip(&#34;&lt;s&gt; \&#39;\&#39; \&#34; &lt;/&#34;), lambda x: re.sub(&#39;&lt;/&#34;&#39;, &#34;&#34;, x))

    import spacy
    nlp = spacy.load(&#34;en_core_web_sm&#34;)
    # Construction via add_pipe with default model
    # sents = nlp(u&#39;A woman is walking through the door.&#39;)

    # for token in sents:
    #     print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_)

    print(&#39;\n[2] - Extracting fillers...&#39;)
    # parsing di tutte le frasi
    head_corpus_v = head_corpus.apply(lambda x: nlp(x)).values

    fillers = []  # [(subj, obj, sentence)]
    for i in range(len(head_corpus_v)):
        sentence = head_corpus_v[i]
        obj = None
        subj = None
        for token in sentence:
            # Se il verbo reggente è il verbo di nostro interesse
            if token.head.text in verb_of_interest:
                # oggetto
                if token.dep == 416:
                    # discarding proper nouns
                    if not token.tag_ == &#39;NNP&#39;:
                        obj = (token.text, token.tag_)
                # soggetto
                elif token.dep == 429:
                    subj = (token.text, token.tag_)
        if obj and subj:
            fillers.append((subj, obj, sentence.text))

    f_length = len(fillers)
    print(f&#34;\n[3] - Total of {f_length} Fillers&#34;)

    our_lesk_semantic_types = {}  # {(s1, s2): count}
    nltk_lesk_semantic_types = {}  # {(s1, s2): count}
    for f in fillers:
        print(&#34;\t{}&#34;.format(f))

        subj_simple = f[0][0]
        subj_tag = f[0][1]
        obj_simple = f[1][0]
        obj_tag = f[1][1]

        # Filtering pronouns
        skip_subj, skipped_s1 = skip_synset_search(pos_tag=subj_tag, t_text=subj_simple)
        skip_obj, skipped_s2 = skip_synset_search(pos_tag=obj_tag, t_text=obj_simple)

        # Vado a chiamare lesk sulla coppia della parte prima della mia frase e complemento oggetto dopo il verbo
        if not skip_subj:
            # Our Lesk
            s1 = lesk(subj_simple, f[2])
            # Wordnet Lesk
            s3 = nltk.wsd.lesk(f[2], subj_simple)
        else:
            s1 = s3 = skipped_s1

        if not skip_obj:
            # Our Lesk
            s2 = lesk(obj_simple, f[2])
            # Wordnet Lesk
            s4 = nltk.wsd.lesk(f[2], obj_simple)
        else:
            s2 = s4 = skipped_s1

        # Our
        update_occurrences(s1, s2, our_lesk_semantic_types)
        # Library
        update_occurrences(s3, s4, nltk_lesk_semantic_types)

    our_lesk_ = &#39;&#34;Our Lesk&#34;&#39;
    fancy_print(&#39;[4.1]&#39; + our_lesk_, our_lesk_semantic_types)

    nltk_lesk_ = &#39;&#34;NLTK Lesk&#34;&#39;
    fancy_print(&#39;[4.2]&#39; + nltk_lesk_, nltk_lesk_semantic_types)

    # Plot
    plot_hanks(our_lesk_semantic_types, 10, f&#34;{our_lesk_} for {verb_of_interest[0]}&#34;)
    plot_hanks(nltk_lesk_semantic_types, 10, f&#34;{nltk_lesk_} for {verb_of_interest[0]}&#34;)


    # Alternative parsing
    # from nltk.parse.corenlp import CoreNLPServer, CoreNLPParser, CoreNLPDependencyParser
    #
    # # The server needs to know the location of the following files:
    # #   - stanford-corenlp-X.X.X.jar
    # #   - stanford-corenlp-X.X.X-models.jar
    # STANFORD = os.path.join(&#34;/home/damians/Scripts/GitHub/&#34;, &#34;stanford-corenlp-4.2.1&#34;)
    #
    # # Create the server
    # jars = (
    #     os.path.join(STANFORD, &#34;stanford-corenlp-4.2.1.jar&#34;),
    #     os.path.join(STANFORD, &#34;stanford-corenlp-4.2.1-models.jar&#34;),
    # )
    #
    # # Start the server in the background
    # with CoreNLPServer(*jars):
    #     # test of parsing
    #     parser = CoreNLPDependencyParser()
    #     parsed_sentence = next(parser.raw_parse(&#34;I put the book in the box on the table.&#34;))
    #     # parsed_sentence.pretty_print()
    #
    #     lemma_df = head_test.apply(lambda x: next(parser.parse(x)))
    #     # print([[(governor, dep, dependent) for governor, dep, dependent in parse.triples()] for parse in lemma])
    #     # lemma_col = lemma.to_conll(4)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="DiCaro.Primo_gruppo.hanks.fancy_print"><code class="name flex">
<span>def <span class="ident">fancy_print</span></span>(<span>title, dictionary)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function for evaluating performance
:param title: string regarding the strategy
:param dictionary: the semantic strategy</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fancy_print(title, dictionary):
    &#34;&#34;&#34;
    Helper function for evaluating performance
    :param title: string regarding the strategy
    :param dictionary: the semantic strategy
    &#34;&#34;&#34;
    print(&#39;\n%s:\n\tFinding Semantic Clusters (percentage, count of instances, semantic cluster):&#39; % title)
    for key, value in sorted(dictionary.items(), key=lambda x: x[1]):
        to_print = str(round((value / f_length) * 100, 2))
        print(&#34;\t[{}%] - {} - {}&#34;.format(to_print, value, key))</code></pre>
</details>
</dd>
<dt id="DiCaro.Primo_gruppo.hanks.get_wordnet_pos"><code class="name flex">
<span>def <span class="ident">get_wordnet_pos</span></span>(<span>tag)</span>
</code></dt>
<dd>
<div class="desc"><p>Map POS tag to first character lemmatize() accepts</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_wordnet_pos(tag):
    &#34;&#34;&#34;Map POS tag to first character lemmatize() accepts&#34;&#34;&#34;
    tag_dict = {&#34;J&#34;: wordnet.ADJ,
                &#34;N&#34;: wordnet.NOUN,
                &#34;V&#34;: wordnet.VERB,
                &#34;R&#34;: wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)</code></pre>
</details>
</dd>
<dt id="DiCaro.Primo_gruppo.hanks.iequal"><code class="name flex">
<span>def <span class="ident">iequal</span></span>(<span>a, b)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper method for comparing tuples with string element
:param a: tuples
:param b: string
:return: True if string in tuples</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def iequal(a, b):
    &#34;&#34;&#34;
    Helper method for comparing tuples with string element
    :param a: tuples
    :param b: string
    :return: True if string in tuples
    &#34;&#34;&#34;
    for value in a:
        try:
            sub = re.sub(&#34;\&#39;&#34;, &#34;&#34;, str(value))
            if b == sub:
                return True
        except AttributeError:
            return value == b</code></pre>
</details>
</dd>
<dt id="DiCaro.Primo_gruppo.hanks.plot_hanks"><code class="name flex">
<span>def <span class="ident">plot_hanks</span></span>(<span>data, max_v, title)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot a horizontal bar graph using pd and matplotlib</p>
<p>:param data: dictionary to bar plot
:param max_v: maximum value of labels
:param title: name of the plot to show</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_hanks(data, max_v, title):
    &#34;&#34;&#34;
    Plot a horizontal bar graph using pd and matplotlib

    :param data: dictionary to bar plot
    :param max_v: maximum value of labels
    :param title: name of the plot to show
    &#34;&#34;&#34;
    plot_df = pd.DataFrame(data, index=[&#39;quantity&#39;])

    # Sort the value
    plot_df = plot_df.sort_values(by=[&#39;quantity&#39;], axis=1, ascending=False)

    # Take top max_v
    top_max = plot_df.iloc[:, :max_v]

    ax = top_max.plot(kind=&#39;barh&#39;, title=f&#34;Hanks cluster with: {title}&#34;, width=8)
    colors, labels = ax.get_legend_handles_labels()

    # envelop in counter
    data = Counter(data)

    # sort the most coomon labels keep only max_v
    best_legend, order = zip(*data.most_common(max_v))
    zipped = zip(colors, labels)
    colors, labels = zip(*[(color, label) for (color, label) in zipped if iequal(best_legend, label)])

    ax.legend(colors, labels, loc=&#39;best&#39;)
    title_f = title.strip().replace(&#34; &#34;, &#34;_&#34;).replace(&#39;\&#34;&#39;, &#34;&#34;)
    plt.savefig(&#39;output/hanks/{}.png&#39;.format(title_f))
    plt.show()
    print(f&#34;\n{title}&#39;s plot saved in output folder.&#34;)</code></pre>
</details>
</dd>
<dt id="DiCaro.Primo_gruppo.hanks.skip_synset_search"><code class="name flex">
<span>def <span class="ident">skip_synset_search</span></span>(<span>pos_tag, t_text)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function for forcing certainty synset to some
:param pos_tag:
:return: boolean value, Synset (or None)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def skip_synset_search(pos_tag, t_text):
    &#34;&#34;&#34;
    Helper function for forcing certainty synset to some
    :param pos_tag:
    :return: boolean value, Synset (or None)
    &#34;&#34;&#34;
    # pronouns
    if pos_tag == &#34;PRP&#34;:
        if t_text.lower() in [&#34;he&#34;, &#34;she&#34;]:
            return True, wordnet.synset(&#39;person.n.01&#39;)
        elif t_text.lower() == &#34;it&#34;:
            return True, wordnet.synset(&#39;artifact.n.01&#39;)
        else:
            return False, None
    # Which / That
    # elif pos_tag == &#34;WDT&#34; or pos_tag == &#34;IN&#34;:
    #     return True, wordnet.synset(&#39;artifact.n.01&#39;)
    else:
        return False, None</code></pre>
</details>
</dd>
<dt id="DiCaro.Primo_gruppo.hanks.update_occurrences"><code class="name flex">
<span>def <span class="ident">update_occurrences</span></span>(<span>ss1, ss2, dictionary)</span>
</code></dt>
<dd>
<div class="desc"><p>Update the dictionary {(s1:s2): counter}
:param ss1: synset1
:param ss2: synset2
:param dictionary: counter dictionary</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_occurrences(ss1, ss2, dictionary):
    &#34;&#34;&#34;
    Update the dictionary {(s1:s2): counter}
    :param ss1: synset1
    :param ss2: synset2
    :param dictionary: counter dictionary
    &#34;&#34;&#34;
    if ss1 is not None and ss2 is not None:
        # Getting supersenses
        t = (ss1.lexname(), ss2.lexname())

        # Getting frequency
        if t in dictionary.keys():
            dictionary[t] = dictionary[t] + 1
        else:
            dictionary[t] = 1</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="DiCaro.Primo_gruppo" href="index.html">DiCaro.Primo_gruppo</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="DiCaro.Primo_gruppo.hanks.fancy_print" href="#DiCaro.Primo_gruppo.hanks.fancy_print">fancy_print</a></code></li>
<li><code><a title="DiCaro.Primo_gruppo.hanks.get_wordnet_pos" href="#DiCaro.Primo_gruppo.hanks.get_wordnet_pos">get_wordnet_pos</a></code></li>
<li><code><a title="DiCaro.Primo_gruppo.hanks.iequal" href="#DiCaro.Primo_gruppo.hanks.iequal">iequal</a></code></li>
<li><code><a title="DiCaro.Primo_gruppo.hanks.plot_hanks" href="#DiCaro.Primo_gruppo.hanks.plot_hanks">plot_hanks</a></code></li>
<li><code><a title="DiCaro.Primo_gruppo.hanks.skip_synset_search" href="#DiCaro.Primo_gruppo.hanks.skip_synset_search">skip_synset_search</a></code></li>
<li><code><a title="DiCaro.Primo_gruppo.hanks.update_occurrences" href="#DiCaro.Primo_gruppo.hanks.update_occurrences">update_occurrences</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>