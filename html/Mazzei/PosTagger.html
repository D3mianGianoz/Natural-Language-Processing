<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>Mazzei.PosTagger API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Mazzei.PosTagger</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import math
import pyconll
import pandas as pd
from collections import Counter

from Mazzei.Smoothing import basic_smooth, baseline

ALMOST_ZERO_P = -99999
LATIN = 42
GREEK = 17

LATIN_DIC = {
    &#34;dev&#34;: &#34;corpus/latin-dev.conllu&#34;,
    &#34;test&#34;: &#34;corpus/latin-test.conllu&#34;,
    &#34;train&#34;: &#34;corpus/latin-train.conllu&#34;
}

GREEK_DIC = {
    &#34;dev&#34;: &#34;corpus/greek-dev.conllu&#34;,
    &#34;test&#34;: &#34;corpus/greek-test.conllu&#34;,
    &#34;train&#34;: &#34;corpus/greek-train.conllu&#34;
}


def num_occurrence(dictionary):
    total_sum = 0
    for elem in dictionary:
        total_sum += dictionary[elem]
    return total_sum


# Debugging printing function
def pretty_print(d, indent=0):
    i = 1
    for key, value in d.items():
        i += 1
        print(&#39;\t&#39; * indent + str(key))
        if isinstance(value, dict):
            pretty_print(value, indent + 2)
        else:
            print(&#39;\t&#39; * (indent + 2) + str(value))


class PosTagger:
    def __init__(self, language: int):
        self.n_tags_given_word = dict()
        self.n_tags_given_tag = dict()

        if language == LATIN:
            self.lang = LATIN_DIC
        elif language == GREEK:
            self.lang = GREEK_DIC
        else:
            raise ValueError(&#39;bad constructor parameter!&#39;, language)

        # 1. Contatori
        self.create_counters(path=self.lang[&#34;train&#34;])
        # 2. Computing probabilities
        self.prob_word_given_tag, self.prob_tag_given_pred_tag = self.calculate_prob()
        # 3. Smoothing/testing
        smoothing_tuple = basic_smooth(path=self.lang[&#34;test&#34;],
                                       emission=self.prob_word_given_tag, tags=self.prob_tag_given_pred_tag)

        self.rating_metrics = {
            &#34;max_noun smoothing&#34;: smoothing_tuple[0],
            &#34;NN_and_VB smoothing&#34;: smoothing_tuple[1],
            &#34;uniform smoothing&#34;: smoothing_tuple[2]
        }

        # 4. Decoding and rating performance
        self.rate(path_to_test=self.lang[&#34;test&#34;])

    def create_counters(self, path: str):
        &#34;&#34;&#34;
        Parsing corpus and counting tags given word and tags given tag adding special tag &#39;START&#39; and &#39;END&#39;.
        We are using collections.Counter() as inner data structure

        Il primo (&#39;START&#39;) è un tag usato per calcolare la probabilità che una parola inizi una frase,
        e che quindi sia la prima. &#39;END&#39; analogamente serve per calcolare la probabilità che una parola
        non sia seguita da nient’altro,ovvero sia alla fine della frase.

        1. Calculating C(ti, wi), n_tags_given_word: counts of tags given specific words
        2. Calculating C(ti-1, ti), n_tags_given_tag: counts of tags given tag
        :param path: path of the corpus
        &#34;&#34;&#34;
        with open(path, &#39;r&#39;) as corpus:
            tag = &#39;&#39;
            prev_tag = &#39;&#39;
            for line in corpus:
                if line[0] == &#39;#&#39;:
                    pass
                # Handle ending of phrases
                if line == &#39;\n&#39;:
                    self.n_tags_given_tag[tag].update({&#39;END&#39;: 1})

                if line.split(&#39;\t&#39;)[0].isdigit():
                    split_line = line.split(&#39;\t&#39;)
                    word = split_line[2]
                    tag = split_line[3]

                    # Calculating C(ti, wi): counts of tags given specific words
                    if word not in self.n_tags_given_word:
                        self.n_tags_given_word[word] = Counter()
                    self.n_tags_given_word[word].update({tag: 1})

                    # Calculating C(ti-1, ti): counts of tags given tag
                    if int(line.split(&#39;\t&#39;)[0]) &gt; 1:
                        if prev_tag not in self.n_tags_given_tag:
                            self.n_tags_given_tag[prev_tag] = Counter()
                        self.n_tags_given_tag[prev_tag].update({tag: 1})

                    # Handling start of phrase
                    if int(line.split(&#39;\t&#39;)[0]) == 1:
                        if &#39;START&#39; not in self.n_tags_given_tag:
                            self.n_tags_given_tag[&#39;START&#39;] = Counter()
                        self.n_tags_given_tag[&#39;START&#39;].update({tag: 1})

                    prev_tag = tag

    def calculate_prob(self):
        &#34;&#34;&#34;
        Creating the statistical model and calculate P(ti | ti-1) and P(wi | ti)
        P(ti | ti-1): probabilità che compaia un tag t dato un tag precedente ti
        P(wi | ti): probabilità che un tag t sia attribuito ad una parola w
        :return:
        &#34;&#34;&#34;
        prob_word_given_tag = self.aux_calculate_prob(self.n_tags_given_word, self.n_tags_given_tag)
        prob_tag_given_pred_tag = self.aux_calculate_prob(self.n_tags_given_tag)

        return prob_word_given_tag, prob_tag_given_pred_tag

    @staticmethod
    def aux_calculate_prob(n_tags_giv: dict, given_tag=None) -&gt; dict:
        &#34;&#34;&#34;
        Iterating through the multi dimension dict we can calculate the natural log of probability.
        La prob è salvata in maniera logaritmica in modo tale da non avere valori troppo piccoli in fase di decoding
        evitando fenomeno di underflow.
        :param given_tag: optional parameter n_tags_given_tag needed when computing emission probability
        :param n_tags_giv: dictionary representing n°tags given either t_1:tag or w:word
        :return: result:dict containing all probabilities (either a_ij o b_ij)
        &#34;&#34;&#34;
        result = dict()
        for element in n_tags_giv:
            for n_tag in n_tags_giv[element]:
                if element not in result:
                    result[element] = dict()
                if given_tag is not None:
                    # Calculate P(wi | ti)
                    p = math.log(n_tags_giv[element][n_tag] / num_occurrence(given_tag[n_tag]))
                else:
                    # Calculate P(ti | ti-1)
                    p = math.log(n_tags_giv[element][n_tag] / num_occurrence(n_tags_giv[element]))
                result[element][n_tag] = p
        return result

    @staticmethod
    def get_test_values(path_to_test):
        &#34;&#34;&#34;
        Retrive from .pyconll files the test set and parse it in appropriate data structure
        :param path_to_test: position on the disk
        :return: test_set: list of phrase, test_pos: list of pos tagging (gold)
        &#34;&#34;&#34;
        test = pyconll.load_from_file(path_to_test)
        test_set = []
        test_pos = []
        counter = -1
        for sentence in test:
            counter += 1
            test_set.append([])
            test_pos.append([])
            for token in sentence:
                test_set[counter].append(token.lemma)
                test_pos[counter].append(token.upos)

        return test_set, test_pos

    def rate(self, path_to_test: str) -&gt; None:
        &#34;&#34;&#34;
        Method to evaluate the performance of our system, final step in the initialization of our class
        :param path_to_test: path of the test set used for evaluation
        &#34;&#34;&#34;
        phrases, correct_tags = self.get_test_values(path_to_test)

        self.rating_metrics[&#34;baseline&#34;] = dict()

        for key, metric in self.rating_metrics.items():
            # Reset for every metrics
            n_correct_pos = 0
            total_n_words = 0

            for j, phrase in enumerate(phrases):
                # TODO statistical smoothing
                total_n_words += len(phrase)
                if key != &#34;baseline&#34;:
                    pos_backpointer, viterbi = self.viterbi_algo(phrase, metric)
                    # pretty_v = pd.DataFrame(viterbi)
                    # print(pretty_v.head())
                    # pretty_print(viterbi, 2)
                else:
                    pos_backpointer = baseline(phrase, self.n_tags_given_word)

                # print(pos_backpointer)
                # pretty_print(pos_backpointer, 2)

                for k, token in enumerate(phrase):
                    if pos_backpointer[token] == correct_tags[j][k]:
                        n_correct_pos += 1

            print(f&#34;Accuracy of technique {key} is: {round((n_correct_pos * 100) / total_n_words, 3)} %&#34;)

    def viterbi_algo(self, phrase: list, smoothed_p: dict):
        &#34;&#34;&#34;
        In una fase preliminare viene inizializzata la prima colonna, per ogni tag, con la somma dei logaritmi
        della probabilità che quel tag sia preceduto dal tag START (probabilità di transizione) e della probabilità
        che alla prima parola della frase sia associato un tale tag (probabilità di emissione). Nel caso non
        esistesse una entry nella probabilità di transizione (default e smoothing) per un tag la cella nella matrice
        corrispondente, allora viene messa a probabilità nulla (ALMOST_ZERO_P).

        Segue poi una fase ricorsiva dove per ogni parola, per ogni possibile tag, vengono calcolate tutte le
        somme in logaritmo delle probabilità riferite alla colonna precedente più la somma di quelle riferite alla
        parola attuale; nella colonna attuale viene salvato solo il valore massimo tra tutte quelle appena calcolate.
        In parallelo viene salvato anche il tag con la probabilità più alta di essere riferito alla parola precedente.
        In questo modo viene tenuta traccia del miglior tag da attribuire ad ogni parola.

        L’ultimo passaggio è analogo al primo, ma fa riferimento al tag LAST andando a completare l’ultima
        colonna della matrice. A causa dell&#39; utilizzo dei log una probabilità nulla non sarà espressa con 0 ma con un
        numero molto piccolo, un questo caso si è scelto ALMOST_ZERO_P, usandolo nella matrice risultante
        Alla fine dell’algoritmo si ottiene, grazie alla matrice di Viterbi, la più probabile taggatura (POS tagging)
        per la frase in input.

        :param phrase: frase da analizzare
        :param smoothed_p: dizionario extra per migliorare le performance
        :return: pos_back_pointer, viterbi
        &#34;&#34;&#34;
        # class variables with more clear and short names
        trans_prob: dict = self.prob_tag_given_pred_tag
        emission_prob: dict = self.prob_word_given_tag

        # 0. local variables
        back_pointer = dict()
        pos_back_pointer = dict()
        viterbi = dict()
        maximum_tag: str = &#39;_&#39;  # init as empty space

        # 1. Initialization step
        hmm_state: str

        for hmm_state in trans_prob.keys():
            if hmm_state != &#39;START&#39;:
                # create the rows of the matrix
                viterbi[hmm_state] = list()
                back_pointer[hmm_state] = list()

                # In case we don&#39;t find we assign 0 probability
                probability: float = ALMOST_ZERO_P

                if hmm_state in trans_prob[&#39;START&#39;]:
                    first_word = phrase[0]
                    if first_word in smoothed_p:
                        if hmm_state in smoothed_p[first_word]:
                            probability = trans_prob[&#39;START&#39;][hmm_state] + smoothed_p[first_word][hmm_state]
                    else:
                        if hmm_state in emission_prob[first_word]:
                            probability = trans_prob[&#39;START&#39;][hmm_state] + emission_prob[first_word][hmm_state]

                # Update the dictionaries key = pos:probability
                viterbi[hmm_state].append(probability)
                back_pointer[hmm_state].append(maximum_tag)

        # 2. Recursion step
        last_t = 0
        for t in range(len(phrase)):
            if t != 0:
                for state in trans_prob.keys():
                    if state != &#39;START&#39;:
                        maximum_tag, bpointer, m_vit = self.maximum(vit=viterbi, word=phrase[t],
                                                                    index=t - 1, m_tag=maximum_tag,
                                                                    current_state=state, smoothed=smoothed_p)

                        #  nella colonna attuale viene salvato solo il valore massimo tra tutte quelle appena calcolate.
                        viterbi[state].append(round(m_vit, 3))
                        back_pointer[state].append(round(bpointer, 3))

                    # il tag con la probabilità più alta di essere riferito alla parola precedente
                    pos_back_pointer[phrase[t - 1]] = maximum_tag
            # salvo l&#39; indice finale
            if t == len(phrase) - 1:
                last_t = t

        # 3. Termination step
        m_path: float = ALMOST_ZERO_P
        last_tag: str = &#39;_&#39;
        for s in trans_prob.keys():
            if s != &#39;START&#39; and &#39;END&#39; in trans_prob[s]:
                temp = viterbi[s][last_t] + trans_prob[s][&#39;END&#39;]
                if temp &gt; m_path:
                    m_path = temp
                    last_tag = s

        # Update dictionary
        viterbi[&#39;END&#39;] = m_path
        pos_back_pointer[phrase[last_t]] = last_tag

        return pos_back_pointer, viterbi

    # Maximum function for viterbi  algorithm
    def maximum(self, vit: dict, word: str, index: int, m_tag: str, current_state: str, smoothed: dict):
        &#34;&#34;&#34;
        Helper function of the viterbi algorithm, it handle the task of
        computing max and argmax for the recursion step

        :param vit: viterbi matrix
        :param word: token that we are currently analizing
        :param index: index of the previous word/column (t-1) important for retrieving probabilities
        :param m_tag: maximum tag founded till now
        :param current_state: HMM state that we are analyzing
        :param smoothed: auxiliary dict for word not present in the training set
        :return: m_tag: max tag founded for probability,
                bpointer: back pointer for reconstructing the path, m_vit: updated viterbi matrix
        &#34;&#34;&#34;

        m_vit, bpointer = ALMOST_ZERO_P, ALMOST_ZERO_P
        # clear and short names
        trans_prob: dict = self.prob_tag_given_pred_tag
        emission_prob: dict = self.prob_word_given_tag

        for state in trans_prob.keys():
            if state != &#39;START&#39;:

                if word in smoothed:
                    # We are gonna use the smoothed probabilities in case of proper nouns
                    temp_dict = smoothed
                else:
                    temp_dict = emission_prob

                if current_state in temp_dict[word] and current_state in trans_prob[state]:
                    temp1 = vit[state][index] + trans_prob[state][current_state] + temp_dict[word][current_state]
                    if temp1 &gt; m_vit:
                        m_vit = temp1

                    temp2 = vit[state][index] + trans_prob[state][current_state]
                    if temp2 &gt; bpointer:
                        bpointer = temp2
                        m_tag = state

        return m_tag, bpointer, m_vit


if __name__ == &#39;__main__&#39;:
    print(&#34;\nPOS tagging Latin LLCT\n&#34;)
    pos_latin = PosTagger(LATIN)
    print(&#34;\nPOS tagging Ancient Greek Perseus \n&#34;)
    pos_greek = PosTagger(GREEK)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="Mazzei.PosTagger.num_occurrence"><code class="name flex">
<span>def <span class="ident">num_occurrence</span></span>(<span>dictionary)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def num_occurrence(dictionary):
    total_sum = 0
    for elem in dictionary:
        total_sum += dictionary[elem]
    return total_sum</code></pre>
</details>
</dd>
<dt id="Mazzei.PosTagger.pretty_print"><code class="name flex">
<span>def <span class="ident">pretty_print</span></span>(<span>d, indent=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pretty_print(d, indent=0):
    i = 1
    for key, value in d.items():
        i += 1
        print(&#39;\t&#39; * indent + str(key))
        if isinstance(value, dict):
            pretty_print(value, indent + 2)
        else:
            print(&#39;\t&#39; * (indent + 2) + str(value))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Mazzei.PosTagger.PosTagger"><code class="flex name class">
<span>class <span class="ident">PosTagger</span></span>
<span>(</span><span>language: int)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PosTagger:
    def __init__(self, language: int):
        self.n_tags_given_word = dict()
        self.n_tags_given_tag = dict()

        if language == LATIN:
            self.lang = LATIN_DIC
        elif language == GREEK:
            self.lang = GREEK_DIC
        else:
            raise ValueError(&#39;bad constructor parameter!&#39;, language)

        # 1. Contatori
        self.create_counters(path=self.lang[&#34;train&#34;])
        # 2. Computing probabilities
        self.prob_word_given_tag, self.prob_tag_given_pred_tag = self.calculate_prob()
        # 3. Smoothing/testing
        smoothing_tuple = basic_smooth(path=self.lang[&#34;test&#34;],
                                       emission=self.prob_word_given_tag, tags=self.prob_tag_given_pred_tag)

        self.rating_metrics = {
            &#34;max_noun smoothing&#34;: smoothing_tuple[0],
            &#34;NN_and_VB smoothing&#34;: smoothing_tuple[1],
            &#34;uniform smoothing&#34;: smoothing_tuple[2]
        }

        # 4. Decoding and rating performance
        self.rate(path_to_test=self.lang[&#34;test&#34;])

    def create_counters(self, path: str):
        &#34;&#34;&#34;
        Parsing corpus and counting tags given word and tags given tag adding special tag &#39;START&#39; and &#39;END&#39;.
        We are using collections.Counter() as inner data structure

        Il primo (&#39;START&#39;) è un tag usato per calcolare la probabilità che una parola inizi una frase,
        e che quindi sia la prima. &#39;END&#39; analogamente serve per calcolare la probabilità che una parola
        non sia seguita da nient’altro,ovvero sia alla fine della frase.

        1. Calculating C(ti, wi), n_tags_given_word: counts of tags given specific words
        2. Calculating C(ti-1, ti), n_tags_given_tag: counts of tags given tag
        :param path: path of the corpus
        &#34;&#34;&#34;
        with open(path, &#39;r&#39;) as corpus:
            tag = &#39;&#39;
            prev_tag = &#39;&#39;
            for line in corpus:
                if line[0] == &#39;#&#39;:
                    pass
                # Handle ending of phrases
                if line == &#39;\n&#39;:
                    self.n_tags_given_tag[tag].update({&#39;END&#39;: 1})

                if line.split(&#39;\t&#39;)[0].isdigit():
                    split_line = line.split(&#39;\t&#39;)
                    word = split_line[2]
                    tag = split_line[3]

                    # Calculating C(ti, wi): counts of tags given specific words
                    if word not in self.n_tags_given_word:
                        self.n_tags_given_word[word] = Counter()
                    self.n_tags_given_word[word].update({tag: 1})

                    # Calculating C(ti-1, ti): counts of tags given tag
                    if int(line.split(&#39;\t&#39;)[0]) &gt; 1:
                        if prev_tag not in self.n_tags_given_tag:
                            self.n_tags_given_tag[prev_tag] = Counter()
                        self.n_tags_given_tag[prev_tag].update({tag: 1})

                    # Handling start of phrase
                    if int(line.split(&#39;\t&#39;)[0]) == 1:
                        if &#39;START&#39; not in self.n_tags_given_tag:
                            self.n_tags_given_tag[&#39;START&#39;] = Counter()
                        self.n_tags_given_tag[&#39;START&#39;].update({tag: 1})

                    prev_tag = tag

    def calculate_prob(self):
        &#34;&#34;&#34;
        Creating the statistical model and calculate P(ti | ti-1) and P(wi | ti)
        P(ti | ti-1): probabilità che compaia un tag t dato un tag precedente ti
        P(wi | ti): probabilità che un tag t sia attribuito ad una parola w
        :return:
        &#34;&#34;&#34;
        prob_word_given_tag = self.aux_calculate_prob(self.n_tags_given_word, self.n_tags_given_tag)
        prob_tag_given_pred_tag = self.aux_calculate_prob(self.n_tags_given_tag)

        return prob_word_given_tag, prob_tag_given_pred_tag

    @staticmethod
    def aux_calculate_prob(n_tags_giv: dict, given_tag=None) -&gt; dict:
        &#34;&#34;&#34;
        Iterating through the multi dimension dict we can calculate the natural log of probability.
        La prob è salvata in maniera logaritmica in modo tale da non avere valori troppo piccoli in fase di decoding
        evitando fenomeno di underflow.
        :param given_tag: optional parameter n_tags_given_tag needed when computing emission probability
        :param n_tags_giv: dictionary representing n°tags given either t_1:tag or w:word
        :return: result:dict containing all probabilities (either a_ij o b_ij)
        &#34;&#34;&#34;
        result = dict()
        for element in n_tags_giv:
            for n_tag in n_tags_giv[element]:
                if element not in result:
                    result[element] = dict()
                if given_tag is not None:
                    # Calculate P(wi | ti)
                    p = math.log(n_tags_giv[element][n_tag] / num_occurrence(given_tag[n_tag]))
                else:
                    # Calculate P(ti | ti-1)
                    p = math.log(n_tags_giv[element][n_tag] / num_occurrence(n_tags_giv[element]))
                result[element][n_tag] = p
        return result

    @staticmethod
    def get_test_values(path_to_test):
        &#34;&#34;&#34;
        Retrive from .pyconll files the test set and parse it in appropriate data structure
        :param path_to_test: position on the disk
        :return: test_set: list of phrase, test_pos: list of pos tagging (gold)
        &#34;&#34;&#34;
        test = pyconll.load_from_file(path_to_test)
        test_set = []
        test_pos = []
        counter = -1
        for sentence in test:
            counter += 1
            test_set.append([])
            test_pos.append([])
            for token in sentence:
                test_set[counter].append(token.lemma)
                test_pos[counter].append(token.upos)

        return test_set, test_pos

    def rate(self, path_to_test: str) -&gt; None:
        &#34;&#34;&#34;
        Method to evaluate the performance of our system, final step in the initialization of our class
        :param path_to_test: path of the test set used for evaluation
        &#34;&#34;&#34;
        phrases, correct_tags = self.get_test_values(path_to_test)

        self.rating_metrics[&#34;baseline&#34;] = dict()

        for key, metric in self.rating_metrics.items():
            # Reset for every metrics
            n_correct_pos = 0
            total_n_words = 0

            for j, phrase in enumerate(phrases):
                # TODO statistical smoothing
                total_n_words += len(phrase)
                if key != &#34;baseline&#34;:
                    pos_backpointer, viterbi = self.viterbi_algo(phrase, metric)
                    # pretty_v = pd.DataFrame(viterbi)
                    # print(pretty_v.head())
                    # pretty_print(viterbi, 2)
                else:
                    pos_backpointer = baseline(phrase, self.n_tags_given_word)

                # print(pos_backpointer)
                # pretty_print(pos_backpointer, 2)

                for k, token in enumerate(phrase):
                    if pos_backpointer[token] == correct_tags[j][k]:
                        n_correct_pos += 1

            print(f&#34;Accuracy of technique {key} is: {round((n_correct_pos * 100) / total_n_words, 3)} %&#34;)

    def viterbi_algo(self, phrase: list, smoothed_p: dict):
        &#34;&#34;&#34;
        In una fase preliminare viene inizializzata la prima colonna, per ogni tag, con la somma dei logaritmi
        della probabilità che quel tag sia preceduto dal tag START (probabilità di transizione) e della probabilità
        che alla prima parola della frase sia associato un tale tag (probabilità di emissione). Nel caso non
        esistesse una entry nella probabilità di transizione (default e smoothing) per un tag la cella nella matrice
        corrispondente, allora viene messa a probabilità nulla (ALMOST_ZERO_P).

        Segue poi una fase ricorsiva dove per ogni parola, per ogni possibile tag, vengono calcolate tutte le
        somme in logaritmo delle probabilità riferite alla colonna precedente più la somma di quelle riferite alla
        parola attuale; nella colonna attuale viene salvato solo il valore massimo tra tutte quelle appena calcolate.
        In parallelo viene salvato anche il tag con la probabilità più alta di essere riferito alla parola precedente.
        In questo modo viene tenuta traccia del miglior tag da attribuire ad ogni parola.

        L’ultimo passaggio è analogo al primo, ma fa riferimento al tag LAST andando a completare l’ultima
        colonna della matrice. A causa dell&#39; utilizzo dei log una probabilità nulla non sarà espressa con 0 ma con un
        numero molto piccolo, un questo caso si è scelto ALMOST_ZERO_P, usandolo nella matrice risultante
        Alla fine dell’algoritmo si ottiene, grazie alla matrice di Viterbi, la più probabile taggatura (POS tagging)
        per la frase in input.

        :param phrase: frase da analizzare
        :param smoothed_p: dizionario extra per migliorare le performance
        :return: pos_back_pointer, viterbi
        &#34;&#34;&#34;
        # class variables with more clear and short names
        trans_prob: dict = self.prob_tag_given_pred_tag
        emission_prob: dict = self.prob_word_given_tag

        # 0. local variables
        back_pointer = dict()
        pos_back_pointer = dict()
        viterbi = dict()
        maximum_tag: str = &#39;_&#39;  # init as empty space

        # 1. Initialization step
        hmm_state: str

        for hmm_state in trans_prob.keys():
            if hmm_state != &#39;START&#39;:
                # create the rows of the matrix
                viterbi[hmm_state] = list()
                back_pointer[hmm_state] = list()

                # In case we don&#39;t find we assign 0 probability
                probability: float = ALMOST_ZERO_P

                if hmm_state in trans_prob[&#39;START&#39;]:
                    first_word = phrase[0]
                    if first_word in smoothed_p:
                        if hmm_state in smoothed_p[first_word]:
                            probability = trans_prob[&#39;START&#39;][hmm_state] + smoothed_p[first_word][hmm_state]
                    else:
                        if hmm_state in emission_prob[first_word]:
                            probability = trans_prob[&#39;START&#39;][hmm_state] + emission_prob[first_word][hmm_state]

                # Update the dictionaries key = pos:probability
                viterbi[hmm_state].append(probability)
                back_pointer[hmm_state].append(maximum_tag)

        # 2. Recursion step
        last_t = 0
        for t in range(len(phrase)):
            if t != 0:
                for state in trans_prob.keys():
                    if state != &#39;START&#39;:
                        maximum_tag, bpointer, m_vit = self.maximum(vit=viterbi, word=phrase[t],
                                                                    index=t - 1, m_tag=maximum_tag,
                                                                    current_state=state, smoothed=smoothed_p)

                        #  nella colonna attuale viene salvato solo il valore massimo tra tutte quelle appena calcolate.
                        viterbi[state].append(round(m_vit, 3))
                        back_pointer[state].append(round(bpointer, 3))

                    # il tag con la probabilità più alta di essere riferito alla parola precedente
                    pos_back_pointer[phrase[t - 1]] = maximum_tag
            # salvo l&#39; indice finale
            if t == len(phrase) - 1:
                last_t = t

        # 3. Termination step
        m_path: float = ALMOST_ZERO_P
        last_tag: str = &#39;_&#39;
        for s in trans_prob.keys():
            if s != &#39;START&#39; and &#39;END&#39; in trans_prob[s]:
                temp = viterbi[s][last_t] + trans_prob[s][&#39;END&#39;]
                if temp &gt; m_path:
                    m_path = temp
                    last_tag = s

        # Update dictionary
        viterbi[&#39;END&#39;] = m_path
        pos_back_pointer[phrase[last_t]] = last_tag

        return pos_back_pointer, viterbi

    # Maximum function for viterbi  algorithm
    def maximum(self, vit: dict, word: str, index: int, m_tag: str, current_state: str, smoothed: dict):
        &#34;&#34;&#34;
        Helper function of the viterbi algorithm, it handle the task of
        computing max and argmax for the recursion step

        :param vit: viterbi matrix
        :param word: token that we are currently analizing
        :param index: index of the previous word/column (t-1) important for retrieving probabilities
        :param m_tag: maximum tag founded till now
        :param current_state: HMM state that we are analyzing
        :param smoothed: auxiliary dict for word not present in the training set
        :return: m_tag: max tag founded for probability,
                bpointer: back pointer for reconstructing the path, m_vit: updated viterbi matrix
        &#34;&#34;&#34;

        m_vit, bpointer = ALMOST_ZERO_P, ALMOST_ZERO_P
        # clear and short names
        trans_prob: dict = self.prob_tag_given_pred_tag
        emission_prob: dict = self.prob_word_given_tag

        for state in trans_prob.keys():
            if state != &#39;START&#39;:

                if word in smoothed:
                    # We are gonna use the smoothed probabilities in case of proper nouns
                    temp_dict = smoothed
                else:
                    temp_dict = emission_prob

                if current_state in temp_dict[word] and current_state in trans_prob[state]:
                    temp1 = vit[state][index] + trans_prob[state][current_state] + temp_dict[word][current_state]
                    if temp1 &gt; m_vit:
                        m_vit = temp1

                    temp2 = vit[state][index] + trans_prob[state][current_state]
                    if temp2 &gt; bpointer:
                        bpointer = temp2
                        m_tag = state

        return m_tag, bpointer, m_vit</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="Mazzei.PosTagger.PosTagger.aux_calculate_prob"><code class="name flex">
<span>def <span class="ident">aux_calculate_prob</span></span>(<span>n_tags_giv: dict, given_tag=None) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Iterating through the multi dimension dict we can calculate the natural log of probability.
La prob è salvata in maniera logaritmica in modo tale da non avere valori troppo piccoli in fase di decoding
evitando fenomeno di underflow.
:param given_tag: optional parameter n_tags_given_tag needed when computing emission probability
:param n_tags_giv: dictionary representing n°tags given either t_1:tag or w:word
:return: result:dict containing all probabilities (either a_ij o b_ij)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def aux_calculate_prob(n_tags_giv: dict, given_tag=None) -&gt; dict:
    &#34;&#34;&#34;
    Iterating through the multi dimension dict we can calculate the natural log of probability.
    La prob è salvata in maniera logaritmica in modo tale da non avere valori troppo piccoli in fase di decoding
    evitando fenomeno di underflow.
    :param given_tag: optional parameter n_tags_given_tag needed when computing emission probability
    :param n_tags_giv: dictionary representing n°tags given either t_1:tag or w:word
    :return: result:dict containing all probabilities (either a_ij o b_ij)
    &#34;&#34;&#34;
    result = dict()
    for element in n_tags_giv:
        for n_tag in n_tags_giv[element]:
            if element not in result:
                result[element] = dict()
            if given_tag is not None:
                # Calculate P(wi | ti)
                p = math.log(n_tags_giv[element][n_tag] / num_occurrence(given_tag[n_tag]))
            else:
                # Calculate P(ti | ti-1)
                p = math.log(n_tags_giv[element][n_tag] / num_occurrence(n_tags_giv[element]))
            result[element][n_tag] = p
    return result</code></pre>
</details>
</dd>
<dt id="Mazzei.PosTagger.PosTagger.get_test_values"><code class="name flex">
<span>def <span class="ident">get_test_values</span></span>(<span>path_to_test)</span>
</code></dt>
<dd>
<div class="desc"><p>Retrive from .pyconll files the test set and parse it in appropriate data structure
:param path_to_test: position on the disk
:return: test_set: list of phrase, test_pos: list of pos tagging (gold)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_test_values(path_to_test):
    &#34;&#34;&#34;
    Retrive from .pyconll files the test set and parse it in appropriate data structure
    :param path_to_test: position on the disk
    :return: test_set: list of phrase, test_pos: list of pos tagging (gold)
    &#34;&#34;&#34;
    test = pyconll.load_from_file(path_to_test)
    test_set = []
    test_pos = []
    counter = -1
    for sentence in test:
        counter += 1
        test_set.append([])
        test_pos.append([])
        for token in sentence:
            test_set[counter].append(token.lemma)
            test_pos[counter].append(token.upos)

    return test_set, test_pos</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="Mazzei.PosTagger.PosTagger.calculate_prob"><code class="name flex">
<span>def <span class="ident">calculate_prob</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Creating the statistical model and calculate P(ti | ti-1) and P(wi | ti)
P(ti | ti-1): probabilità che compaia un tag t dato un tag precedente ti
P(wi | ti): probabilità che un tag t sia attribuito ad una parola w
:return:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_prob(self):
    &#34;&#34;&#34;
    Creating the statistical model and calculate P(ti | ti-1) and P(wi | ti)
    P(ti | ti-1): probabilità che compaia un tag t dato un tag precedente ti
    P(wi | ti): probabilità che un tag t sia attribuito ad una parola w
    :return:
    &#34;&#34;&#34;
    prob_word_given_tag = self.aux_calculate_prob(self.n_tags_given_word, self.n_tags_given_tag)
    prob_tag_given_pred_tag = self.aux_calculate_prob(self.n_tags_given_tag)

    return prob_word_given_tag, prob_tag_given_pred_tag</code></pre>
</details>
</dd>
<dt id="Mazzei.PosTagger.PosTagger.create_counters"><code class="name flex">
<span>def <span class="ident">create_counters</span></span>(<span>self, path: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Parsing corpus and counting tags given word and tags given tag adding special tag 'START' and 'END'.
We are using collections.Counter() as inner data structure</p>
<p>Il primo ('START') è un tag usato per calcolare la probabilità che una parola inizi una frase,
e che quindi sia la prima. 'END' analogamente serve per calcolare la probabilità che una parola
non sia seguita da nient’altro,ovvero sia alla fine della frase.</p>
<ol>
<li>Calculating C(ti, wi), n_tags_given_word: counts of tags given specific words</li>
<li>Calculating C(ti-1, ti), n_tags_given_tag: counts of tags given tag
:param path: path of the corpus</li>
</ol></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_counters(self, path: str):
    &#34;&#34;&#34;
    Parsing corpus and counting tags given word and tags given tag adding special tag &#39;START&#39; and &#39;END&#39;.
    We are using collections.Counter() as inner data structure

    Il primo (&#39;START&#39;) è un tag usato per calcolare la probabilità che una parola inizi una frase,
    e che quindi sia la prima. &#39;END&#39; analogamente serve per calcolare la probabilità che una parola
    non sia seguita da nient’altro,ovvero sia alla fine della frase.

    1. Calculating C(ti, wi), n_tags_given_word: counts of tags given specific words
    2. Calculating C(ti-1, ti), n_tags_given_tag: counts of tags given tag
    :param path: path of the corpus
    &#34;&#34;&#34;
    with open(path, &#39;r&#39;) as corpus:
        tag = &#39;&#39;
        prev_tag = &#39;&#39;
        for line in corpus:
            if line[0] == &#39;#&#39;:
                pass
            # Handle ending of phrases
            if line == &#39;\n&#39;:
                self.n_tags_given_tag[tag].update({&#39;END&#39;: 1})

            if line.split(&#39;\t&#39;)[0].isdigit():
                split_line = line.split(&#39;\t&#39;)
                word = split_line[2]
                tag = split_line[3]

                # Calculating C(ti, wi): counts of tags given specific words
                if word not in self.n_tags_given_word:
                    self.n_tags_given_word[word] = Counter()
                self.n_tags_given_word[word].update({tag: 1})

                # Calculating C(ti-1, ti): counts of tags given tag
                if int(line.split(&#39;\t&#39;)[0]) &gt; 1:
                    if prev_tag not in self.n_tags_given_tag:
                        self.n_tags_given_tag[prev_tag] = Counter()
                    self.n_tags_given_tag[prev_tag].update({tag: 1})

                # Handling start of phrase
                if int(line.split(&#39;\t&#39;)[0]) == 1:
                    if &#39;START&#39; not in self.n_tags_given_tag:
                        self.n_tags_given_tag[&#39;START&#39;] = Counter()
                    self.n_tags_given_tag[&#39;START&#39;].update({tag: 1})

                prev_tag = tag</code></pre>
</details>
</dd>
<dt id="Mazzei.PosTagger.PosTagger.maximum"><code class="name flex">
<span>def <span class="ident">maximum</span></span>(<span>self, vit: dict, word: str, index: int, m_tag: str, current_state: str, smoothed: dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function of the viterbi algorithm, it handle the task of
computing max and argmax for the recursion step</p>
<p>:param vit: viterbi matrix
:param word: token that we are currently analizing
:param index: index of the previous word/column (t-1) important for retrieving probabilities
:param m_tag: maximum tag founded till now
:param current_state: HMM state that we are analyzing
:param smoothed: auxiliary dict for word not present in the training set
:return: m_tag: max tag founded for probability,
bpointer: back pointer for reconstructing the path, m_vit: updated viterbi matrix</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def maximum(self, vit: dict, word: str, index: int, m_tag: str, current_state: str, smoothed: dict):
    &#34;&#34;&#34;
    Helper function of the viterbi algorithm, it handle the task of
    computing max and argmax for the recursion step

    :param vit: viterbi matrix
    :param word: token that we are currently analizing
    :param index: index of the previous word/column (t-1) important for retrieving probabilities
    :param m_tag: maximum tag founded till now
    :param current_state: HMM state that we are analyzing
    :param smoothed: auxiliary dict for word not present in the training set
    :return: m_tag: max tag founded for probability,
            bpointer: back pointer for reconstructing the path, m_vit: updated viterbi matrix
    &#34;&#34;&#34;

    m_vit, bpointer = ALMOST_ZERO_P, ALMOST_ZERO_P
    # clear and short names
    trans_prob: dict = self.prob_tag_given_pred_tag
    emission_prob: dict = self.prob_word_given_tag

    for state in trans_prob.keys():
        if state != &#39;START&#39;:

            if word in smoothed:
                # We are gonna use the smoothed probabilities in case of proper nouns
                temp_dict = smoothed
            else:
                temp_dict = emission_prob

            if current_state in temp_dict[word] and current_state in trans_prob[state]:
                temp1 = vit[state][index] + trans_prob[state][current_state] + temp_dict[word][current_state]
                if temp1 &gt; m_vit:
                    m_vit = temp1

                temp2 = vit[state][index] + trans_prob[state][current_state]
                if temp2 &gt; bpointer:
                    bpointer = temp2
                    m_tag = state

    return m_tag, bpointer, m_vit</code></pre>
</details>
</dd>
<dt id="Mazzei.PosTagger.PosTagger.rate"><code class="name flex">
<span>def <span class="ident">rate</span></span>(<span>self, path_to_test: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Method to evaluate the performance of our system, final step in the initialization of our class
:param path_to_test: path of the test set used for evaluation</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rate(self, path_to_test: str) -&gt; None:
    &#34;&#34;&#34;
    Method to evaluate the performance of our system, final step in the initialization of our class
    :param path_to_test: path of the test set used for evaluation
    &#34;&#34;&#34;
    phrases, correct_tags = self.get_test_values(path_to_test)

    self.rating_metrics[&#34;baseline&#34;] = dict()

    for key, metric in self.rating_metrics.items():
        # Reset for every metrics
        n_correct_pos = 0
        total_n_words = 0

        for j, phrase in enumerate(phrases):
            # TODO statistical smoothing
            total_n_words += len(phrase)
            if key != &#34;baseline&#34;:
                pos_backpointer, viterbi = self.viterbi_algo(phrase, metric)
                # pretty_v = pd.DataFrame(viterbi)
                # print(pretty_v.head())
                # pretty_print(viterbi, 2)
            else:
                pos_backpointer = baseline(phrase, self.n_tags_given_word)

            # print(pos_backpointer)
            # pretty_print(pos_backpointer, 2)

            for k, token in enumerate(phrase):
                if pos_backpointer[token] == correct_tags[j][k]:
                    n_correct_pos += 1

        print(f&#34;Accuracy of technique {key} is: {round((n_correct_pos * 100) / total_n_words, 3)} %&#34;)</code></pre>
</details>
</dd>
<dt id="Mazzei.PosTagger.PosTagger.viterbi_algo"><code class="name flex">
<span>def <span class="ident">viterbi_algo</span></span>(<span>self, phrase: list, smoothed_p: dict)</span>
</code></dt>
<dd>
<div class="desc"><p>In una fase preliminare viene inizializzata la prima colonna, per ogni tag, con la somma dei logaritmi
della probabilità che quel tag sia preceduto dal tag START (probabilità di transizione) e della probabilità
che alla prima parola della frase sia associato un tale tag (probabilità di emissione). Nel caso non
esistesse una entry nella probabilità di transizione (default e smoothing) per un tag la cella nella matrice
corrispondente, allora viene messa a probabilità nulla (ALMOST_ZERO_P).</p>
<p>Segue poi una fase ricorsiva dove per ogni parola, per ogni possibile tag, vengono calcolate tutte le
somme in logaritmo delle probabilità riferite alla colonna precedente più la somma di quelle riferite alla
parola attuale; nella colonna attuale viene salvato solo il valore massimo tra tutte quelle appena calcolate.
In parallelo viene salvato anche il tag con la probabilità più alta di essere riferito alla parola precedente.
In questo modo viene tenuta traccia del miglior tag da attribuire ad ogni parola.</p>
<p>L’ultimo passaggio è analogo al primo, ma fa riferimento al tag LAST andando a completare l’ultima
colonna della matrice. A causa dell' utilizzo dei log una probabilità nulla non sarà espressa con 0 ma con un
numero molto piccolo, un questo caso si è scelto ALMOST_ZERO_P, usandolo nella matrice risultante
Alla fine dell’algoritmo si ottiene, grazie alla matrice di Viterbi, la più probabile taggatura (POS tagging)
per la frase in input.</p>
<p>:param phrase: frase da analizzare
:param smoothed_p: dizionario extra per migliorare le performance
:return: pos_back_pointer, viterbi</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def viterbi_algo(self, phrase: list, smoothed_p: dict):
    &#34;&#34;&#34;
    In una fase preliminare viene inizializzata la prima colonna, per ogni tag, con la somma dei logaritmi
    della probabilità che quel tag sia preceduto dal tag START (probabilità di transizione) e della probabilità
    che alla prima parola della frase sia associato un tale tag (probabilità di emissione). Nel caso non
    esistesse una entry nella probabilità di transizione (default e smoothing) per un tag la cella nella matrice
    corrispondente, allora viene messa a probabilità nulla (ALMOST_ZERO_P).

    Segue poi una fase ricorsiva dove per ogni parola, per ogni possibile tag, vengono calcolate tutte le
    somme in logaritmo delle probabilità riferite alla colonna precedente più la somma di quelle riferite alla
    parola attuale; nella colonna attuale viene salvato solo il valore massimo tra tutte quelle appena calcolate.
    In parallelo viene salvato anche il tag con la probabilità più alta di essere riferito alla parola precedente.
    In questo modo viene tenuta traccia del miglior tag da attribuire ad ogni parola.

    L’ultimo passaggio è analogo al primo, ma fa riferimento al tag LAST andando a completare l’ultima
    colonna della matrice. A causa dell&#39; utilizzo dei log una probabilità nulla non sarà espressa con 0 ma con un
    numero molto piccolo, un questo caso si è scelto ALMOST_ZERO_P, usandolo nella matrice risultante
    Alla fine dell’algoritmo si ottiene, grazie alla matrice di Viterbi, la più probabile taggatura (POS tagging)
    per la frase in input.

    :param phrase: frase da analizzare
    :param smoothed_p: dizionario extra per migliorare le performance
    :return: pos_back_pointer, viterbi
    &#34;&#34;&#34;
    # class variables with more clear and short names
    trans_prob: dict = self.prob_tag_given_pred_tag
    emission_prob: dict = self.prob_word_given_tag

    # 0. local variables
    back_pointer = dict()
    pos_back_pointer = dict()
    viterbi = dict()
    maximum_tag: str = &#39;_&#39;  # init as empty space

    # 1. Initialization step
    hmm_state: str

    for hmm_state in trans_prob.keys():
        if hmm_state != &#39;START&#39;:
            # create the rows of the matrix
            viterbi[hmm_state] = list()
            back_pointer[hmm_state] = list()

            # In case we don&#39;t find we assign 0 probability
            probability: float = ALMOST_ZERO_P

            if hmm_state in trans_prob[&#39;START&#39;]:
                first_word = phrase[0]
                if first_word in smoothed_p:
                    if hmm_state in smoothed_p[first_word]:
                        probability = trans_prob[&#39;START&#39;][hmm_state] + smoothed_p[first_word][hmm_state]
                else:
                    if hmm_state in emission_prob[first_word]:
                        probability = trans_prob[&#39;START&#39;][hmm_state] + emission_prob[first_word][hmm_state]

            # Update the dictionaries key = pos:probability
            viterbi[hmm_state].append(probability)
            back_pointer[hmm_state].append(maximum_tag)

    # 2. Recursion step
    last_t = 0
    for t in range(len(phrase)):
        if t != 0:
            for state in trans_prob.keys():
                if state != &#39;START&#39;:
                    maximum_tag, bpointer, m_vit = self.maximum(vit=viterbi, word=phrase[t],
                                                                index=t - 1, m_tag=maximum_tag,
                                                                current_state=state, smoothed=smoothed_p)

                    #  nella colonna attuale viene salvato solo il valore massimo tra tutte quelle appena calcolate.
                    viterbi[state].append(round(m_vit, 3))
                    back_pointer[state].append(round(bpointer, 3))

                # il tag con la probabilità più alta di essere riferito alla parola precedente
                pos_back_pointer[phrase[t - 1]] = maximum_tag
        # salvo l&#39; indice finale
        if t == len(phrase) - 1:
            last_t = t

    # 3. Termination step
    m_path: float = ALMOST_ZERO_P
    last_tag: str = &#39;_&#39;
    for s in trans_prob.keys():
        if s != &#39;START&#39; and &#39;END&#39; in trans_prob[s]:
            temp = viterbi[s][last_t] + trans_prob[s][&#39;END&#39;]
            if temp &gt; m_path:
                m_path = temp
                last_tag = s

    # Update dictionary
    viterbi[&#39;END&#39;] = m_path
    pos_back_pointer[phrase[last_t]] = last_tag

    return pos_back_pointer, viterbi</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Mazzei" href="index.html">Mazzei</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="Mazzei.PosTagger.num_occurrence" href="#Mazzei.PosTagger.num_occurrence">num_occurrence</a></code></li>
<li><code><a title="Mazzei.PosTagger.pretty_print" href="#Mazzei.PosTagger.pretty_print">pretty_print</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Mazzei.PosTagger.PosTagger" href="#Mazzei.PosTagger.PosTagger">PosTagger</a></code></h4>
<ul class="two-column">
<li><code><a title="Mazzei.PosTagger.PosTagger.aux_calculate_prob" href="#Mazzei.PosTagger.PosTagger.aux_calculate_prob">aux_calculate_prob</a></code></li>
<li><code><a title="Mazzei.PosTagger.PosTagger.calculate_prob" href="#Mazzei.PosTagger.PosTagger.calculate_prob">calculate_prob</a></code></li>
<li><code><a title="Mazzei.PosTagger.PosTagger.create_counters" href="#Mazzei.PosTagger.PosTagger.create_counters">create_counters</a></code></li>
<li><code><a title="Mazzei.PosTagger.PosTagger.get_test_values" href="#Mazzei.PosTagger.PosTagger.get_test_values">get_test_values</a></code></li>
<li><code><a title="Mazzei.PosTagger.PosTagger.maximum" href="#Mazzei.PosTagger.PosTagger.maximum">maximum</a></code></li>
<li><code><a title="Mazzei.PosTagger.PosTagger.rate" href="#Mazzei.PosTagger.PosTagger.rate">rate</a></code></li>
<li><code><a title="Mazzei.PosTagger.PosTagger.viterbi_algo" href="#Mazzei.PosTagger.PosTagger.viterbi_algo">viterbi_algo</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>