{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING WITH RNN AND LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Text\n",
    "\n",
    "Neural networks don't take raw text data as an input. This means we need to encode textual data to numeric values that the model can understand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore when we encode words, we would like to recognize similar and different to catch similarity among them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "To solve this issue we can use **word embeddings**. This method keeps the order of words intact as well as encodes similar words with very similar labels. It attempts to not only encode the frequency and order of words but the meaning of those words in the sentence. It encodes each word as a dense vector that represents its context in the sentence.\n",
    "\n",
    "Unlike classical and logical methods, word embeddings are learned by looking at many different training examples. You can add what's called an *embedding layer* to the begining of a neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN's)\n",
    "\n",
    "### Recurrent Neural Networks\n",
    "\n",
    "RNN's are a kind of neural network that is pretty capable of processing sequential data such as text. \n",
    "\n",
    "I will consider the folling topics:\n",
    "- Sentiment Analysis\n",
    "- Character Generation \n",
    "\n",
    "RNN's are complex and come in many different forms so I wil focus on how they work and the kind of problems they are best suited for.\n",
    "\n",
    "When dealing with texts, it is not quite usefull to keep tha data all at once. After all, even we (humans) don't process text all at once. We read word by word from left to right and keep track of the current meaning of the sentence so we can understand the meaning of the next word. Well this is exaclty what a recurrent neural network is designed to do. When we say recurrent neural network all we really mean is a network that contains a loop. A RNN will process one word at a time while maintaining an internal memory of what it's already seen. This will allow it to treat words differently based on their order in a sentence and to slowly build an understanding of the entire input, one word at a time.\n",
    "\n",
    "This is why we are treating our text data as a sequence! So that we can pass one word at a time to the RNN.\n",
    "\n",
    "Let's have a look at what a recurrent layer might look like.\n",
    "\n",
    "![alt text](RNN.png)\n",
    "\n",
    "*Source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/*\n",
    "\n",
    "Let's define what all these variables stand for before we get into the explination.\n",
    "\n",
    "**h<sub>t</sub>** output at time t\n",
    "\n",
    "**x<sub>t</sub>** input at time t\n",
    "\n",
    "**A** Recurrent Layer (loop)\n",
    "\n",
    "A recurrent layer processes words or input one at a time in a combination with the output from the previous iteration. So, as we progress further in the input sequence, we build a more complex understanding of the text as a whole.\n",
    "\n",
    "What we've just looked at is called a **simple RNN layer**. It can be effective at processing shorter sequences of text but there could be some problems when sentences became longer. In fact, it gets increasingly difficult for the network to understand the text properly when it gets longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "However, there exist some other recurrent layers that can solve this problem such as the LSTM (Long Short-Term Memory). This layer works quite similarily to the simpleRNN layer but adds a way to access inputs from any timestep in the past. While in the simple RNN layer input from previous timestamps gradually disappeared as we got further through the input, with a LSTM we have a long-term memory data structure storing all the previously seen inputs as well as when we saw them. This allows for us to access any previous value we want at any point in time. This also allows to discover more useful relationships between inputs and when they appear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "*It is the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.*\n",
    "(Wikipedia)\n",
    "\n",
    "The example I considered here is abount classifying movie reviews as positive, negative or neutral.\n",
    "\n",
    "*This work is based on the following tensorflow tutorial: https://www.tensorflow.org/tutorials/text/text_classification_rnn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Dataset\n",
    "\n",
    "I will use the IMDB movie review dataset from keras. This dataset contains 25,000 reviews from IMDB where each one is already preprocessed and has a label as either positive or negative. Each review is encoded by integers that represents how common a word is in the entire dataset. For example, a word encoded by the integer 3 means that it is the 3rd most common word in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.datasets import imdb #loading the IMDB movie review dataset from keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "VOCAB_SIZE = 88584 #number of unique words\n",
    "\n",
    "MAXLEN = 250 #max lenght considered for a review\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words have the first review in the training set\n",
    "len(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#labels tell me how a review is classified \n",
    "for i in range(10):\n",
    "    print(train_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in train_labels:\n",
    "    if i == 1:\n",
    "        counter += 1\n",
    "print(counter)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "#maximum and minimum lenght of a review\n",
    "lenreviewlist = []\n",
    "for i in range(len(train_data)):\n",
    "    lenreviewlist.append(len(train_data[i]))\n",
    "    \n",
    "print(max(lenreviewlist))\n",
    "print(min(lenreviewlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238.71364"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average lenght of a review\n",
    "from statistics import mean\n",
    "\n",
    "media = mean(lenreviewlist)\n",
    "media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8431 occurrences above the mean!\n",
      "There are 16569 occurrences below the mean!\n"
     ]
    }
   ],
   "source": [
    "#Let see the distribution of lenghts around the mean\n",
    "counter1 = 0\n",
    "counter2 = 0\n",
    "for ln in lenreviewlist:\n",
    "    if ln >= round(media):\n",
    "        counter1 += 1\n",
    "    else:\n",
    "        counter2 += 1\n",
    "        \n",
    "print(\"There are \" + str(counter1) + \" occurrences above the mean!\")\n",
    "print(\"There are \" + str(counter2) + \" occurrences below the mean!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7879 occurrences above MAXLEN!\n",
      "There are 17121 occurrences below MAXLEN!\n"
     ]
    }
   ],
   "source": [
    "#Let see the distribution of lenghts around the MAXLEN chosen\n",
    "counter1 = 0\n",
    "counter2 = 0\n",
    "for ln in lenreviewlist:\n",
    "    if ln >= MAXLEN:\n",
    "        counter1 += 1\n",
    "    else:\n",
    "        counter2 += 1\n",
    "        \n",
    "print(\"There are \" + str(counter1) + \" occurrences above MAXLEN!\")\n",
    "print(\"There are \" + str(counter2) + \" occurrences below MAXLEN!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lenght distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(lenreviewlist, bins=100) \n",
    "# Add title and axis names\n",
    "plt.xlabel('lenghts of a review')\n",
    "plt.ylabel('occurrences')\n",
    "plt.title(\"histogram\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Preprocessing\n",
    "Looking at some reviews, we have seen that they have different lengths. This is an issue. We cannot pass different length data into our neural network. Therefore, we must make each review the same length. To do this we will follow the procedure below:\n",
    "\n",
    "- I chosed a MAXLEN = 250 to set the common lenght of our reviews\n",
    "- if the review is greater than 250 words then trim off the extra words\n",
    "- if the review is less than 250 words add the necessary amount of 0's to make it equal to 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE: all reviews have the same lenght\n"
     ]
    }
   ],
   "source": [
    "#CHECK\n",
    "counter = 0\n",
    "for i in range(len(train_data)):\n",
    "    if len(train_data[i]) == 250 and len(test_data[i]) == 250:\n",
    "        counter += 1\n",
    "if counter == len(train_data):\n",
    "    print('TRUE: all reviews have the same lenght')\n",
    "else:\n",
    "    print('FALSE: something went wrong')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model\n",
    "To create the model I will use a word embedding layer as the first layer and add a LSTM layer afterwards that feeds into a dense node to get our predicted sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example of a Sequential model that processes sequences of integers, embeds each integer into a 32-dimensional vector, then processes the sequence of vectors using a LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, None, 32)          2834688   \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,843,041\n",
      "Trainable params: 2,843,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "# Add an Embedding layer expecting input vocab of size VOCAB_SIZE, and\n",
    "# output embedding dimension of size 32.\n",
    "# 32: output dimension of the vectors generated by the embedding layer \n",
    "# (it can be changed in 64 for example)\n",
    "model.add(layers.Embedding(input_dim=VOCAB_SIZE, output_dim=32))\n",
    "\n",
    "# Add a LSTM layer with 128 internal units.\n",
    "model.add(layers.LSTM(32))\n",
    "\n",
    "# Add a Dense layer with 1 units.\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 48s 77ms/step - loss: 0.4232 - acc: 0.8065 - val_loss: 0.2867 - val_acc: 0.8840\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 48s 77ms/step - loss: 0.2402 - acc: 0.9089 - val_loss: 0.2670 - val_acc: 0.8938\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 45s 72ms/step - loss: 0.1832 - acc: 0.9329 - val_loss: 0.3026 - val_acc: 0.8874\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 45s 72ms/step - loss: 0.1547 - acc: 0.9449 - val_loss: 0.2913 - val_acc: 0.8900\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 45s 73ms/step - loss: 0.1281 - acc: 0.9560 - val_loss: 0.2965 - val_acc: 0.8928\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 47s 76ms/step - loss: 0.1100 - acc: 0.9625 - val_loss: 0.3182 - val_acc: 0.8874\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 48s 76ms/step - loss: 0.0981 - acc: 0.9678 - val_loss: 0.4686 - val_acc: 0.8620\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 48s 76ms/step - loss: 0.0870 - acc: 0.9704 - val_loss: 0.3846 - val_acc: 0.8554\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 48s 77ms/step - loss: 0.0787 - acc: 0.9750 - val_loss: 0.3407 - val_acc: 0.8906\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 48s 78ms/step - loss: 0.0692 - acc: 0.9775 - val_loss: 0.4549 - val_acc: 0.8642\n"
     ]
    }
   ],
   "source": [
    "#training first model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['acc'])\n",
    "\n",
    "#we use 20% of training data to validate\n",
    "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 13s 16ms/step - loss: 0.6163 - acc: 0.8175\n",
      "[0.6163066029548645, 0.817520022392273]\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "Now letâ€™s use our network to make predictions on our own reviews. \n",
    "\n",
    "Since our reviews are encoded we'll need to convert any review that we write into that form so the network can understand it. To do that well load the encodings from the dataset and use them to encode our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index() #gets parameters of imdb\n",
    "\n",
    "#convert a text in the same format of imdb\n",
    "def encode_text(text):\n",
    "    '''\n",
    "    Encode text in the good format for the neural network\n",
    "    '''\n",
    "    tokens = keras.preprocessing.text.text_to_word_sequence(text)\n",
    "    tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
    "    return sequence.pad_sequences([tokens], MAXLEN)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
     ]
    }
   ],
   "source": [
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#while were at it lets make a decode function\n",
    "#from integer to words\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_text(integers):\n",
    "    '''\n",
    "    Decode a list of integer into a text\n",
    "    '''\n",
    "    PAD = 0\n",
    "    text = \"\"\n",
    "    for num in integers:\n",
    "        if num != PAD: #I want to skip zeros\n",
    "            text += reverse_word_index[num] + \" \"\n",
    "\n",
    "    return text[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that movie was just amazing so amazing\n"
     ]
    }
   ],
   "source": [
    "print(decode_text(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a prediction\n",
    "#if 1 is a good prediction, if 0 is a bad prediction\n",
    "\n",
    "def predict(text):\n",
    "    '''\n",
    "    Make a prediction about new reviews\n",
    "    '''\n",
    "    encoded_text = encode_text(text)\n",
    "    pred = np.zeros((1,250)) #I create a list of 250 zeros\n",
    "    pred[0] = encoded_text #I fullfill the list with the ecncoded text\n",
    "    result = model.predict(pred) #I apply my model to predict the text\n",
    "    print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8992923]\n",
      "[0.19408754]\n"
     ]
    }
   ],
   "source": [
    "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
    "predict(positive_review)\n",
    "\n",
    "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
    "predict(negative_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39208558]\n"
     ]
    }
   ],
   "source": [
    "neg1 = 'This movie was very bad. I have never seen anything so disgusting and howful.'\n",
    "predict(neg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-d704b354f28d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpos1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bad'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "pos1 = 'bad'\n",
    "predict(pos1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Play Generator\n",
    "\n",
    "Now I am going to use a RNN for text generation. I will simply show to the RNN an example of something I want it to recreate and it will learn how to write a version of it on its own. I'll do this using a character predictive model that will take as input a variable length sequence and predict the next character.\n",
    "\n",
    "\n",
    "*Based on the following: https://www.tensorflow.org/tutorials/text/text_generation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "For the training I am going to use the shakespeare.txt file in keras, which is an extract from a shakesphere play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "#Read, then decode.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "#length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Take a look at the first 250 characters\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this text isn't encoded yet, I need to do it. I am going to create a dictionary encoding each unique character as a different integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text)) #set of sorted unique characters\n",
    "\n",
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)} #discrionary of simbols\n",
    "idx2char = np.array(vocab) #array of simbols\n",
    "\n",
    "def text_to_int(text):\n",
    "    '''\n",
    "    unction that can convert our text to integer values\n",
    "    '''\n",
    "    return np.array([char2idx[c] for c in text])\n",
    "\n",
    "#convert text to int\n",
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "#looking at the dictionary\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: First Citizen\n",
      "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# lets look at how part of our text is encoded\n",
    "# now each character is an integer number\n",
    "print(\"Text:\", text[:13])\n",
    "print(\"Encoded:\", text_to_int(text[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "#function that can convert our numeric values to text\n",
    "def int_to_text(ints):\n",
    "    '''\n",
    "    Function that can convert our numeric values to text\n",
    "    '''\n",
    "    try:\n",
    "        ints = ints.numpy()\n",
    "    except:\n",
    "        pass\n",
    "    return ''.join(idx2char[ints])\n",
    "\n",
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training Examples\n",
    "To solve our task we need to split the text data from above into many shorter sequences that we can pass to the model as training examples. \n",
    "\n",
    "The training examples we will prepare will use a *seq_length* sequence as input and a *seq_length* sequence as the output where that sequence is the original sequence shifted one letter to the right. For example:\n",
    "\n",
    "```input: Firs | output: irst```\n",
    "\n",
    "The first step will be to create a stream of characters from the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100  # length of sequence for a training example\n",
    "examples_per_epoch = len(text)//(seq_length+1) #just a fraction of the entire dataset\n",
    "\n",
    "#Create training examples / targets\n",
    "#each element is something like tf.Tensor(18, shape=(), dtype=int64)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can use the batch method to turn this stream of characters into batches of desired length\n",
    "#it will return a tensor of lenght seq_length+1\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1], shape=(101,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "#The first sequence is:\n",
    "for i in sequences.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use these sequences and split them into input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):  # for the example: First\n",
    "    '''\n",
    "    Splits a sequence\n",
    "    '''\n",
    "    input_text = chunk[:-1]  # firs\n",
    "    target_text = chunk[1:]  # irst\n",
    "    return input_text, target_text  # firs, irst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['F', 'i', 'r', 's'], ['i', 'r', 's', 't'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Applicative example\n",
    "\n",
    "split_input_target(list(\"First\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use map to apply the above function to every entry\n",
    "dataset = sequences.map(split_input_target)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43,\n",
      "       44, 53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39,\n",
      "       52, 63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1,\n",
      "       51, 43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31,\n",
      "       54, 43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56,\n",
      "       57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "       53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52,\n",
      "       63,  1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51,\n",
      "       43,  1, 57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54,\n",
      "       43, 39, 49,  6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57,\n",
      "       58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1])>)\n"
     ]
    }
   ],
   "source": [
    "#What this mean applied to our dataset (at the first sequence for example)\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "\n",
      "OUTPUT\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      "EXAMPLE\n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "\n",
      "OUTPUT\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for inp, out in dataset.take(2):\n",
    "    print(\"\\n\\nEXAMPLE\\n\")\n",
    "    print(\"INPUT\")\n",
    "    print(int_to_text(inp))\n",
    "    print(\"\\nOUTPUT\")\n",
    "    print(int_to_text(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make training batches\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n",
    "EMBEDDING_DIM = 256 #dimensionality of the vector who represents my words\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "#Buffer size to shuffle the dataset\n",
    "#TF data is designed to work with possibly infinite sequences,\n",
    "#so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "#it maintains a buffer in which it shuffles elements.\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "To build the model I am going to use as embedding layer a LSTM and one dense layer that contains a node for each unique character in our training data. The dense layer will give us a probability distribution over all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                             return_sequences=True,\n",
    "                             stateful=True,\n",
    "                             recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "      ])\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Loss Function\n",
    "Now we are going to create our own loss function. This is because our model will output a (64, sequence_length, 65) shaped tensor that represents the probability distribution of each character at each timestep for every sequence in the batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) ---> (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "#Having a look at a sample input and the output from our untrained model. \n",
    "#This is so we can understand what the model is giving us:\n",
    "\n",
    "#This is so we can understand what the model is giving us\n",
    "\n",
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "    #ask our model for a prediction on our first batch of training data (64 entries)\n",
    "    example_batch_predictions = model(input_example_batch)  \n",
    "    #print out the output shape\n",
    "    print(example_batch_predictions.shape, \"---> (batch_size, sequence_length, vocab_size)\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "#we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n",
    "print(len(example_batch_predictions))\n",
    "print(len(example_batch_predictions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.00445798  0.00134858 -0.00162167 ... -0.00028184 -0.0039648\n",
      "   0.00053552]\n",
      " [ 0.00089486 -0.00240264 -0.00456375 ...  0.00299994 -0.00367627\n",
      "   0.00339394]\n",
      " [ 0.0051625  -0.00305768 -0.00127644 ...  0.00161988 -0.00073513\n",
      "   0.00845567]\n",
      " ...\n",
      " [ 0.00495043 -0.00049978 -0.00193697 ...  0.0014215  -0.00419908\n",
      "   0.00390788]\n",
      " [ 0.00098599 -0.00077373 -0.00172263 ... -0.00185591 -0.00192765\n",
      "   0.00776092]\n",
      " [-0.00578503  0.00924841  0.00202241 ... -0.0007435  -0.00247671\n",
      "   0.00286253]], shape=(100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# lets examine one prediction\n",
    "pred = example_batch_predictions[0]\n",
    "print(pred)\n",
    "# notice this is a 2d array of length 100, where each interior array \n",
    "# is the prediction for the next character at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "tf.Tensor(\n",
      "[ 0.00445798  0.00134858 -0.00162167  0.00209351 -0.00392073  0.00720163\n",
      "  0.00102119 -0.00258099 -0.00530009 -0.00631695  0.00344318 -0.00220213\n",
      "  0.00614996 -0.00286755  0.00014293 -0.00032655 -0.00147814 -0.00224423\n",
      "  0.00483373 -0.00361421  0.00014439  0.00262165  0.00301029  0.00091313\n",
      " -0.00163417  0.00902739 -0.00292949  0.00148445 -0.00478405  0.00686833\n",
      " -0.00228322  0.00301745  0.00074382 -0.00712284  0.00256641  0.00537799\n",
      " -0.0006014  -0.0055846  -0.00523396 -0.00258584  0.00342558 -0.00560113\n",
      "  0.00080947 -0.00215131 -0.0002083   0.00402516  0.00501857 -0.00042529\n",
      " -0.00158762  0.00646292 -0.00061794  0.00415354 -0.00078896 -0.00030841\n",
      "  0.0023119   0.00079     0.00071945 -0.00119077  0.00604747  0.00401607\n",
      " -0.00467812 -0.00131532 -0.00028184 -0.0039648   0.00053552], shape=(65,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# and finally well look at a prediction at the first timestep\n",
    "time_pred = pred[0]\n",
    "print(len(time_pred))\n",
    "print(time_pred)\n",
    "# and of course its 65 values representing the probability of each character occuring next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45, 18, 48, 62, 51, 36,  1, 60, 30,  4,  9, 63, 40, 35, 50, 54, 17,\n",
       "       27, 23, 48, 60, 15, 48, 45,  4,  0, 29, 44, 51, 57, 53, 35, 54, 46,\n",
       "       35, 41,  9, 30,  7,  3, 45, 14, 53, 50, 15,  5, 50, 47, 56, 54, 20,\n",
       "       38, 12, 19, 47, 40, 18, 32, 34,  4, 42, 51, 40, 57, 26, 48, 12, 32,\n",
       "       59, 16, 10, 60, 25, 44,  3, 28, 54, 10,  0, 26, 57, 42, 54, 30, 54,\n",
       "        8, 36, 37, 40, 50, 15, 15, 18, 39, 40,  4, 49, 45, 60,  9])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "\n",
    "# now we can reshape that array and convert all the integers to text to see the actual characters\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"gFjxmX vR&3ybWlpEOKjvCjg&\\nQfmsoWphWc3R-$gBolC'lirpHZ?GibFTV&dmbsNj?TuD:vMf$Pp:\\nNsdpRp.XYblCCFab&kgv3\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars  # and this is what the model predicted for training sequence 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "#sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "#sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "#redicted_chars  # and this is what the model predicted for training sequence 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we need to create a loss function that can compare that output to the expected output and give us some numeric value representing how close the two were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    #Because our model returns logits, you need to set the from_logits flag to True.\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss:  4.1743603\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Mean loss: \", mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.99168"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE: output logits have similar magniture\n"
     ]
    }
   ],
   "source": [
    "r = 2 #fix range\n",
    "\n",
    "if VOCAB_SIZE - r < tf.exp(mean_loss).numpy() < VOCAB_SIZE + r:\n",
    "    print('TRUE: output logits have similar magniture')\n",
    "else:\n",
    "    print('FALSE: badly initialization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Model\n",
    "At this point we can think of our problem as a classification problem where the model predicts the probabillity of each unique letter coming next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure the training procedure using the tf.keras.Model.compile method. \n",
    "#Use tf.keras.optimizers.Adam with default arguments and the loss function.\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIGURE CHECKPOINT:\n",
    "\n",
    "Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-167-639792f8ff28>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-167-639792f8ff28>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    N_epochs =  #number of epochs of training (for computational problems)\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "N_epochs = 1 #number of epochs of training (for computational problems)\n",
    "#history = model.fit(data, epochs=N_epochs, callbacks=[checkpoint_callback])\n",
    "history = model.fit(data, epochs=N_epochs, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#I load data from the checkpoit without training again\n",
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 800\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "    \n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: o pandy:\n",
      "Mron has beedel is I my theek shear is youg ffage.\n",
      "\n",
      "MOBGUEEY:,\n",
      "N'tt Meait his lave I as nat no he horese.\n",
      "\n",
      "RWORUSES:\n",
      "Y se rond, stared Meare,\n",
      "This, And day groo I re thye ase farpadsers on bese be as, she ca me;\n",
      "\n",
      "PUWINCOS:\n",
      "Siching ast of vime awerf\n",
      "Wo reece dyar!\n",
      "\n",
      "PINENTEO:\n",
      "Sy lot of so wack doss\n",
      "For setare he ouk Hefomed, bost th thapl sonfon sererlole.\n",
      "\n",
      "Ars IfNRUUS:\n",
      "I,\n",
      "Het'l is anw a mate wol preen pringint begther: Sean the\n",
      "henter makewst he canty;\n",
      "Ise saye hos the row leaste:\n",
      "A hard eavemnort in Ciraebut, and it Custreese\n",
      "Nut ther Hecarse at thy srrematince?\n",
      "Bum EFpains:\n",
      "I sheld I co he vorw:\n",
      "O way ghyse heme ap. swate my of me the forsens\n",
      "'thang er greas bio hisf my our buss.\n",
      "\n",
      "RWARY,\n",
      "Be,\n",
      "Gy firth seen-\n",
      "Mate yourath my ary the buste\n",
      "Wacners af govers, and whot That as thes ast\n"
     ]
    }
   ],
   "source": [
    "start = 'ROMEO: '\n",
    "print(generate_text(model, start_string = start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Type a starting string:  ROMEO:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Bhes hers yours, he in thue, bester? me wat vear wour I gise whoodss wore ow searse but if to nod d saingiod so thit\n",
      "What wens h'd your Iull ald.\n",
      "\n",
      "LIONYUCSUS:\n",
      "Thent\n",
      "ENRSEENA\n",
      "NABDARBER:\n",
      "Bat affar? Low beat vimp lave ho will ro' injusate to theare; whace nogors blenime lis,\n",
      "Alb Setan\n",
      "You dot and Josher you!\n",
      "Ad loor the ripfore de thee un bleile.\n",
      "\n",
      "KANG HIVCAn RANI Godraro I'averx, Co flougtered an mence a't heve beter nots are\n",
      "an I put mald in the came:\n",
      "Sy note st sast of, ald the ded-angay!\n",
      "Shes lave thee-Larile I gamen? It co wele twem oel hind the crnoag. The lace now sore.\n",
      "\n",
      "MERYOI:\n",
      "Y wease go slast as; Sa bliet sthef hplame sutsings to shue ands, fram, lo thear shayver a my ceonger.\n",
      "Stor thet here fornow the gith he,\n",
      "I foret: she hoth ut inde ma the\n",
      "erting menth cenchathas,\n",
      "wion whas ble\n"
     ]
    }
   ],
   "source": [
    "inp = input(\"Type a starting string: \")\n",
    "print(generate_text(model, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower's I menof\n",
      "P:\n",
      "Geron that hald I nuf be noth of tule and dair ant alds be:\n",
      "Yeil someriengs, andallt sa,\n",
      "And slat roder bear!\n",
      "Buthing an gating thes shith sit?\n",
      "Yous lok,\n",
      "\n",
      "IONZES:\n",
      "He then Il ur you, as sipover hith his, hake frirster?\n",
      "O rangel soodst,\n",
      "The thew wliging se in, the follt grorsssend of coofur.\n",
      "\n",
      "RISWARDENBIO:\n",
      "'d suts that you shale?\n",
      "\n",
      "PEOTEO:\n",
      "Nowe shearme fost, Ly thes weor than wead wame, af be the ncoowat\n",
      "Andour'ds rrean leath muss moferes kenk\n",
      "Mantrage youe his cromes of a wim my lain and thich\n",
      "O'ljoully bround be tak shae e and s kndeate makest lasion's! aw gorst to the thave praght hears, sach, is feack hit hoth may,\n",
      "Ont you is llogher nus is and his look;\n",
      "Brather: my suesf as Lame Tombon!\n",
      "I RLOUK:\n",
      "Lo herene com came; and this y whet awrory wett cain.\n",
      "\n",
      "NONESEES:\n",
      "This lamy my ot succladnt if,\n",
      "And Sother, le he onasper his whese; dith llave is nattin,\n",
      "Sw thone lot vord'mage us nath stout ke of dy ghave the aeg;\n",
      "Maveed, thour bemelabe fo reathw Lorangre,\n",
      "The negnen?\n",
      "\n",
      "KESPESLA\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
    "    \n",
    "    # Number of characters to generate\n",
    "    num_generate = gen_size\n",
    "    # Vecotrizing starting seed text\n",
    "    input_eval = [char2idx[s] for s in start_seed]\n",
    "    # Expand to match batch format shape\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    # Empty list to hold resulting generated text\n",
    "    text_generated = []\n",
    "    # Temperature effects randomness in our resulting text\n",
    "    # The term is derived from entropy/thermodynamics.\n",
    "    # The temperature is used to effect probability of next characters.\n",
    "    # Higher probability == lesss surprising/ more expected\n",
    "    # Lower temperature == more surprising / less expected\n",
    "    temperature = temp\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        \n",
    "        # Generate Predictions\n",
    "        predictions = model(input_eval)\n",
    "        # Remove the batch shape dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # Use a cateogircal disitribution to select the next character\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "        # Pass the predicted charracter for the next input\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        # Transform back to character letter\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    return (start_seed + ''.join(text_generated))\n",
    "\n",
    "print(generate_text(model,\"flower\",gen_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "1. Chollet FranÃ§ois. Deep Learning with Python. Manning Publications Co., 2018.\n",
    "2. â€œText Classification with an RNN &nbsp;: &nbsp; TensorFlow Core.â€ TensorFlow, www.tensorflow.org/tutorials/text/text_classification_rnn.\n",
    "3. â€œText Generation with an RNN &nbsp;: &nbsp; TensorFlow Core.â€ TensorFlow, www.tensorflow.org/tutorials/text/text_generation.\n",
    "4. â€œUnderstanding LSTM Networks.â€ Understanding LSTM Networks -- Colah's Blog, https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n",
    "5. \"Working with RNN's\"\n",
    "https://keras.io/guides/working_with_rnns/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
